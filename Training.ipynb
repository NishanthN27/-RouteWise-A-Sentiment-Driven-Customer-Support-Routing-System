{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "collapsed": true,
        "id": "GsuezIpX13s5"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "import tensorflow as tf\n",
        "from transformers import TFAutoModel, AutoTokenizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "import pandas as pd\n",
        "from keras.saving import register_keras_serializable\n",
        "from keras.models import load_model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "mxYKLuv35xBo"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Load your custom dataset\n",
        "data = pd.read_csv('Dataset.csv')\n",
        "\n",
        "# Map sentiment labels to integers\n",
        "label_map = {\n",
        "    \"Satisfied\": 0,\n",
        "    \"Neutral\": 1,\n",
        "    \"Slightly Dissatisfied\": 2,\n",
        "    \"Dissatisfied\": 3,\n",
        "    \"Highly Dissatisfied\": 4\n",
        "}\n",
        "data['label'] = data['label'].map(label_map)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MVUPyb9k-AvC",
        "outputId": "6fcee263-1e1b-4ca5-8578-d942944546fb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "10\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "print(data['text'].isna().sum())\n",
        "print(data['label'].isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "EkYN5hWD9pxB"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "data = data.dropna(subset=['text', 'label'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AND2j9pn9_9N",
        "outputId": "699d55a0-7c88-4ae7-caf1-5f68ff23c787"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n",
            "0\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "print(data['text'].isna().sum())\n",
        "print(data['label'].isna().sum())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "wdBEG0YH51Gs"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "# Perform stratified train-validation-test split (80-10-10 split)\n",
        "train_texts, temp_texts, train_labels, temp_labels = train_test_split(\n",
        "    data['text'], data['label'], test_size=0.2, random_state=42, stratify=data['label']\n",
        ")\n",
        "\n",
        "val_texts, test_texts, val_labels, test_labels = train_test_split(\n",
        "    temp_texts, temp_labels, test_size=0.5, random_state=42, stratify=temp_labels\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wSZcD8j550W",
        "outputId": "e5184eb1-143c-4f4f-ba33-6c67ca57058f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Class distribution in training set:\n",
            "label\n",
            "1.0    0.213511\n",
            "3.0    0.208580\n",
            "4.0    0.207840\n",
            "2.0    0.186391\n",
            "0.0    0.183679\n",
            "Name: proportion, dtype: float64\n",
            "Class distribution in validation set:\n",
            "label\n",
            "1.0    0.213018\n",
            "3.0    0.209073\n",
            "4.0    0.207101\n",
            "2.0    0.187377\n",
            "0.0    0.183432\n",
            "Name: proportion, dtype: float64\n",
            "Class distribution in test set:\n",
            "label\n",
            "1.0    0.214567\n",
            "4.0    0.208661\n",
            "3.0    0.208661\n",
            "2.0    0.185039\n",
            "0.0    0.183071\n",
            "Name: proportion, dtype: float64\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# Verify class distribution\n",
        "print(\"Class distribution in training set:\")\n",
        "print(pd.Series(train_labels).value_counts(normalize=True))\n",
        "\n",
        "print(\"Class distribution in validation set:\")\n",
        "print(pd.Series(val_labels).value_counts(normalize=True))\n",
        "\n",
        "print(\"Class distribution in test set:\")\n",
        "print(pd.Series(test_labels).value_counts(normalize=True))\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "2CnC2i3n58hO"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "def tokenize(texts, labels):\n",
        "    tokenized = tokenizer(list(texts), padding=True, truncation=True, return_tensors='tf')\n",
        "    tokenized_inputs = {\n",
        "        'input_ids': tf.cast(tokenized['input_ids'], dtype=tf.int32),\n",
        "        'attention_mask': tf.cast(tokenized['attention_mask'], dtype=tf.int32),\n",
        "        'token_type_ids': tf.cast(tokenized['token_type_ids'], dtype=tf.int32),\n",
        "    }\n",
        "    tokenized_labels = tf.convert_to_tensor(labels.astype(int), dtype=tf.int32)\n",
        "    return tokenized_inputs, tokenized_labels\n",
        "\n",
        "\n",
        "# Now try to tokenize again\n",
        "train_data = tokenize(train_texts, train_labels)\n",
        "val_data = tokenize(val_texts, val_labels)\n",
        "test_data = tokenize(test_texts, test_labels)\n",
        "\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices(train_data).batch(32)\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices(val_data).batch(32)\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices(test_data).batch(32)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQ15dFbzYuS8",
        "outputId": "7ba513ac-a8a2-477d-d403-9a5e70b38bdd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "input_ids dtype: <dtype: 'int32'>\n",
            "attention_mask dtype: <dtype: 'int32'>\n",
            "token_type_ids dtype: <dtype: 'int32'>\n",
            "label dtype: <dtype: 'int32'>\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "inputs, labels = train_data  # Unpack the tuple\n",
        "print(\"input_ids dtype:\", inputs['input_ids'].dtype)\n",
        "print(\"attention_mask dtype:\", inputs['attention_mask'].dtype)\n",
        "print(\"token_type_ids dtype:\", inputs['token_type_ids'].dtype)\n",
        "print(\"label dtype:\", labels.dtype)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oFsfQuZ36Bp8",
        "outputId": "b7fc481d-fd83-4d6a-dcd6-a5c2081a0700"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Input batch: {'input_ids': <tf.Tensor: shape=(32, 30), dtype=int32, numpy=\n",
            "array([[  101,   146,  1444,  6043,  1389,  1665,  1114,  5321,  1126,\n",
            "         8926,  1106,  1546,   102,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1109,  3317,  1110,  1171,  1106,  1684,  6150,   119,\n",
            "         5438,  1111,  1103,  3613,  8239,   106,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,   146,   112,   182,  1378,  1146,  1106,  1267,  1191,\n",
            "         1139,  1231, 14703,  3276,  4566,  1110,  1971,  1223,  3189,\n",
            "          119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1494,  1106, 24295,  1126,  3342,  1104,  1546,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,   146,  2222,  1106,  5782,  1199,  4237,  1121,  1546,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1370,  1842,   117,  1103,  5467, 19722,  1158,  1180,\n",
            "          112,  1396,  1151,  1236,  1167, 11824,   119,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1188,  1110,  9944,   106,   146,  3004,  1111,  6779,\n",
            "          117,  1105,  1208,   146,   112,   182,  6705,  1114,  1142,\n",
            "        12178,   106,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101, 24107,  9944,   106,  1555, 14915,  8513,  1110,  1253,\n",
            "         8362,  4894,  4063,  5790,  1170,  1429,  1552,   119,  2091,\n",
            "          146,  1444,  1106, 13803,  1208,   136,   102,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101, 13227,  1105, 10682,  1619,  1111,  1139,  9366,  1394,\n",
            "         2463,   119,  1277, 12503,   106,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  4302,  1866,  1149,  1164, 19722,  1979,  1139, 16759,\n",
            "          119,  1122,  1108,  1198,  1126,  1903,  2541,   119,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,   146,   112,   182,  7351,  1114,  1103,  3613,  8239,\n",
            "          132,  1103,  3265,  1110,  1684,  6150,  1208,   119,   102,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  5268,  1454,  1149,  1618,  1190,   146,  1518,  1180,\n",
            "         1138,  7683,   119,   102,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1109,  1555,  1108, 15354,   117,  1133,  1103,  4909,\n",
            "         1334,  2204,  1603,   119,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1109,  5166,  1127, 12095,   117,  1133,   146,  1445,\n",
            "          112,   189,  3665,  2816,  1114,  1172,   119,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,   146,   112,   182,  3196, 14355,   106,  1731,  1169,\n",
            "         1128,  6477,  1146,  1254,  1113,  1103,  1269,  2486,   136,\n",
            "          102,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  4081,  1159,   146,  2367,  1164,  1292,  9334,  4917,\n",
            "          117,   146,  1243,  1185,  1842,  7108,   119,  1188,  1110,\n",
            "         7284, 18970,   106,   102,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,   152, 14666,   117,  1240,  1619,  1264, 22797,   119,\n",
            "         4938,  8926,  1460,  1144,  1151, 12482,  1113,  1111,   160,\n",
            "        27073, 25370,  1208,   119, 17355,  1775,  1122, 24819,  2924,\n",
            "          106,   102,     0],\n",
            "       [  101,  1109,  1546,  1261,  5221,  1106,  1437,  1146,   117,\n",
            "        21884,  9333,   119,   102,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,   146,  3004,  1111,  6848,  8629,   117,  1133,  1139,\n",
            "         1546,  2474,  1523,  4050,   119,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,   146,   112,   182,  5737,  1114,  1103,  8130,  1104,\n",
            "         1139,  4938,  8926,  2486,   119,  5055,  5663,  2109,  2168,\n",
            "          119,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1109,  1546,  2474,   117,  1133,  1103,  8629,  1159,\n",
            "         1445,   112,   189,  7891,   119,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,   146,   112,   182,  1136, 17278,  1164,  1103,  8513,\n",
            "         1107,  4172,  1139,  7305,   119,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,   178,  1274,  1204,  1328, 12327, 23632,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  8653,  5671,  1104,  1139,  1159,   102,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,   146,  1821,  1774,  1106,  5194,  1126,  3342,  1106,\n",
            "         1546,   102,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,   146,   136,   182,  2776,  9333,  1114,  1103,  4550,\n",
            "         1158,  6223,   117,  1133,  1122,   136,   188,  1136,   170,\n",
            "         3321,  2463,   119,   102,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1135,   112,   188,  2423,  8362,  1643,  2180,  8124,\n",
            "        16656,  1348,  1106,  1817,  1143,  5205,  1114,  1185,  2330,\n",
            "         6615,  1164,  1139,  9334,  4917,   106,   102,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101, 23686,   117,  1354,  1103,  1965,  1156,  1129, 21104,\n",
            "          117,  1133,  1185,  3186,   119,   102,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,   146,  2637,  1618,  1555,   119,  3300, 18011,  1108,\n",
            "         2071, 14247,   119,   102,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,  1753,  1103,  1436,  3879,   117,  1133,  1103,  4938,\n",
            "         8926,  1110,  1253,  3008,  1106,  1329,   119,   102,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,   146,  1138,  1400,  1106, 14609,  4779,   117,  1180,\n",
            "         1128,  1494,  1143,   136,   102,     0,     0,     0,     0,\n",
            "            0,     0,     0,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0],\n",
            "       [  101,   146,  1274,   112,   189,  1328,  1106, 19073,   117,\n",
            "         1133,  4550,  1158,  7353,  1108,  1136,  8630, 12486,  1120,\n",
            "         1155,   119,   102,     0,     0,     0,     0,     0,     0,\n",
            "            0,     0,     0]], dtype=int32)>, 'attention_mask': <tf.Tensor: shape=(32, 30), dtype=int32, numpy=\n",
            "array([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 1, 1, 1, 1, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
            "        1, 1, 1, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>, 'token_type_ids': <tf.Tensor: shape=(32, 30), dtype=int32, numpy=\n",
            "array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0],\n",
            "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
            "        0, 0, 0, 0, 0, 0, 0, 0]], dtype=int32)>}\n",
            "Output labels: tf.Tensor([1 0 1 1 1 3 4 4 0 1 0 0 2 2 4 4 4 2 3 3 2 2 1 4 1 2 4 2 3 2 1 3], shape=(32,), dtype=int32)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n",
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# Verify a batch from the train dataset\n",
        "inp, out = next(iter(train_dataset))\n",
        "print(\"Input batch:\", inp)\n",
        "print(\"Output labels:\", out)\n",
        "\n",
        "# Load the pre-trained BERT model\n",
        "base_model = TFAutoModel.from_pretrained(\"bert-base-cased\")\n",
        "\n",
        "# Define a classification model using BERT\n",
        "# Define a classification model using BERT\n",
        "@register_keras_serializable(package=\"Custom\")\n",
        "class BERTForClassification(tf.keras.Model):\n",
        "    def __init__(self, bert_model, num_classes, **kwargs):\n",
        "        super().__init__(**kwargs)\n",
        "        self.bert = bert_model\n",
        "        self.fc = tf.keras.layers.Dense(num_classes, activation='softmax')\n",
        "\n",
        "    def call(self, inputs):\n",
        "        x = self.bert(inputs)[1]  # Extract pooled output (last hidden state)\n",
        "        return self.fc(x)\n",
        "\n",
        "    def get_config(self):\n",
        "        config = super(BERTForClassification, self).get_config()\n",
        "        config.update({\n",
        "            'num_classes': self.fc.units,\n",
        "            'bert_model_name': self.bert.name_or_path\n",
        "        })\n",
        "        return config\n",
        "\n",
        "    @classmethod\n",
        "    def from_config(cls, config):\n",
        "        bert_model = TFAutoModel.from_pretrained(config[\"bert_model_name\"])\n",
        "        return cls(bert_model=bert_model, num_classes=config[\"num_classes\"])\n",
        "\n",
        "# This will be used when creating and saving the model:\n",
        "def create_model():\n",
        "    base_model = TFAutoModel.from_pretrained(\"bert-base-cased\")\n",
        "    model = BERTForClassification(base_model, num_classes=5)\n",
        "\n",
        "    # Specify input signatures\n",
        "    input_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"input_ids\")\n",
        "    attention_mask = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"attention_mask\")\n",
        "    token_type_ids = tf.keras.Input(shape=(None,), dtype=tf.int32, name=\"token_type_ids\")\n",
        "\n",
        "    # Call the model with input signatures\n",
        "    outputs = model({\n",
        "        \"input_ids\": input_ids,\n",
        "        \"attention_mask\": attention_mask,\n",
        "        \"token_type_ids\": token_type_ids\n",
        "    })\n",
        "\n",
        "    return model\n",
        "\n",
        "classifier = create_model()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IeSpExNy6DsY",
        "outputId": "3bdfaca2-f133-493d-d5bf-0da8b21f2ea9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "127/127 [==============================] - 65s 243ms/step - loss: 0.7383 - accuracy: 0.7392 - val_loss: 0.2978 - val_accuracy: 0.9093\n",
            "Epoch 2/3\n",
            "127/127 [==============================] - 28s 220ms/step - loss: 0.2268 - accuracy: 0.9248 - val_loss: 0.2471 - val_accuracy: 0.9053\n",
            "Epoch 3/3\n",
            "127/127 [==============================] - 28s 216ms/step - loss: 0.1246 - accuracy: 0.9625 - val_loss: 0.2027 - val_accuracy: 0.9250\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# Compile the model\n",
        "classifier.compile(\n",
        "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
        "    loss=tf.keras.losses.SparseCategoricalCrossentropy(),\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = classifier.fit(\n",
        "    train_dataset,\n",
        "    validation_data=val_dataset,\n",
        "    epochs=3,\n",
        "    callbacks=[\n",
        "        tf.keras.callbacks.EarlyStopping(\n",
        "            monitor='val_loss', patience=2, restore_best_weights=True\n",
        "        )\n",
        "    ]\n",
        ")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6pfyVX-76FcR",
        "outputId": "da2af1f3-f1d4-4de3-c00a-a5ec772a9dca"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 4s 77ms/step - loss: 0.2197 - accuracy: 0.9193\n",
            "Test Loss: 0.2196902334690094, Test Accuracy: 0.9192913174629211\n",
            "Enter a sentence to classify (or type 'exit' to quit): amazingly bad\n",
            "Predicted class: Dissatisfied (Confidence: 0.73)\n",
            "Enter a sentence to classify (or type 'exit' to quit): pathetic\n",
            "Predicted class: Highly Dissatisfied (Confidence: 0.89)\n",
            "Enter a sentence to classify (or type 'exit' to quit): absolute shit\n",
            "Predicted class: Highly Dissatisfied (Confidence: 0.96)\n",
            "Enter a sentence to classify (or type 'exit' to quit): this was absolutely shit service\n",
            "Predicted class: Highly Dissatisfied (Confidence: 0.90)\n",
            "Enter a sentence to classify (or type 'exit' to quit): i'm amazed at how disastrous this is\n",
            "Predicted class: Dissatisfied (Confidence: 0.49)\n",
            "Enter a sentence to classify (or type 'exit' to quit): disastrous\n",
            "Predicted class: Highly Dissatisfied (Confidence: 0.72)\n",
            "Enter a sentence to classify (or type 'exit' to quit): hmm\n",
            "Predicted class: Neutral (Confidence: 0.94)\n",
            "Enter a sentence to classify (or type 'exit' to quit): i have issues\n",
            "Predicted class: Neutral (Confidence: 1.00)\n",
            "Enter a sentence to classify (or type 'exit' to quit): please help me fix this shit. absolutely nonsensical how u have set this product up\n",
            "Predicted class: Highly Dissatisfied (Confidence: 0.95)\n",
            "Enter a sentence to classify (or type 'exit' to quit): horrible service\n",
            "Predicted class: Highly Dissatisfied (Confidence: 0.97)\n",
            "Enter a sentence to classify (or type 'exit' to quit): waiter was rude and food was abysmal\n",
            "Predicted class: Dissatisfied (Confidence: 0.81)\n",
            "Enter a sentence to classify (or type 'exit' to quit): bad bad good good\n",
            "Predicted class: Neutral (Confidence: 0.54)\n",
            "Enter a sentence to classify (or type 'exit' to quit): good bad\n",
            "Predicted class: Neutral (Confidence: 0.96)\n",
            "Enter a sentence to classify (or type 'exit' to quit): huh??!!\n",
            "Predicted class: Highly Dissatisfied (Confidence: 0.77)\n",
            "Enter a sentence to classify (or type 'exit' to quit): how tf\n",
            "Predicted class: Neutral (Confidence: 1.00)\n",
            "Enter a sentence to classify (or type 'exit' to quit): tf is this service\n",
            "Predicted class: Neutral (Confidence: 1.00)\n",
            "Enter a sentence to classify (or type 'exit' to quit): exit\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "# Evaluate the model on the test dataset\n",
        "test_loss, test_accuracy = classifier.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")\n",
        "\n",
        "# Manual input for classification\n",
        "def classify_manual_input():\n",
        "    while True:\n",
        "        user_input = input(\"Enter a sentence to classify (or type 'exit' to quit): \")\n",
        "        if user_input.lower() == 'exit':\n",
        "            break\n",
        "        inputs = tokenizer(user_input, return_tensors=\"tf\", padding=True, truncation=True)\n",
        "        prediction = classifier(inputs)[0].numpy()\n",
        "        predicted_class = prediction.argmax()\n",
        "        for key, value in label_map.items():\n",
        "            if value == predicted_class:\n",
        "                print(f\"Predicted class: {key} (Confidence: {prediction[predicted_class]:.2f})\")\n",
        "\n",
        "classify_manual_input()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "-uoZwQZsmt_f"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "classifier.save(\"Nishanth_saved_model\", save_format=\"tf\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o2Ddgp0em_gu",
        "outputId": "fa72559a-9321-4453-92d1-250c39dcd5be"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the PyTorch model were not used when initializing the TF 2.0 model TFBertModel: ['cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight']\n",
            "- This IS expected if you are initializing TFBertModel from a PyTorch model trained on another task or with another architecture (e.g. initializing a TFBertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from a PyTorch model that you expect to be exactly identical (e.g. initializing a TFBertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the weights of TFBertModel were initialized from the PyTorch model.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "loaded_model = tf.keras.models.load_model(\n",
        "    \"Nishanth_saved_model\",\n",
        "    custom_objects={\"BERTForClassification\": BERTForClassification}\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7RtYSu4dnTMD",
        "outputId": "353b8490-594c-4a5a-a42c-7a9d99bbda57"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "16/16 [==============================] - 8s 93ms/step - loss: 0.2234 - accuracy: 0.9252\n",
            "Test Loss: 0.2233937382698059, Test Accuracy: 0.9251968264579773\n"
          ]
        }
      ],
      "source": [
        "# @title\n",
        "test_loss, test_accuracy = loaded_model.evaluate(test_dataset)\n",
        "print(f\"Test Loss: {test_loss}, Test Accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "21BaEKQWoRw8",
        "outputId": "c4a99ed0-5978-4462-f25f-e5321024d43d"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/Nishanth_saved_model.zip'"
            ]
          },
          "execution_count": 34,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# @title\n",
        "import shutil\n",
        "shutil.make_archive(\"/content/Nishanth_saved_model\", 'zip', \"/content/Nishanth_saved_model\")\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
